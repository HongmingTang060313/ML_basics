{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading MNIST Train Dataset\n",
    "**Images from 1 to 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms\n",
    "transforms = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri_train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms,\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the primary training set to a new training set and validation set\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(pri_train_dataset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change Image size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "# Pass the datasets to the data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modified_Lenet_5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Modified_Lenet_5, self).__init__()   \n",
    "        # ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "        # Max pool\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        # Batch norm 0\n",
    "        self.bn0 = nn.BatchNorm2d(1)\n",
    "        # Batch norm 1\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        # Batch norm 2 \n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        # Batch norm 3\n",
    "        self.bn3 = nn.BatchNorm2d(120)\n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1,padding=2)\n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        # Convolution 3\n",
    "        self.cnn3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n",
    "        # Fully connected 1 \n",
    "        self.fc1 = nn.Linear(120, 84) \n",
    "        # Fully connected 2 (Readout)\n",
    "        self.fc2 = nn.Linear(84,10)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Li+18: weight layer -> BN -> FC -> Activation\n",
    "        # Chen+19: Before CN1, do BN+FC. Moreover, do BN+FC before every weight layer. (BN -> FC -> weight layer -> Activation)\n",
    "        # Convolution 1\n",
    "        #x = self.bn0(x)\n",
    "        #x = self.dropout(x)\n",
    "        out = self.cnn1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        # Max pool 1\n",
    "        out = self.maxpool(out)\n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # Max pool 2 \n",
    "        out = self.maxpool(out)\n",
    "        # Convolution 3\n",
    "        out = self.cnn3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        # Resize\n",
    "        out = out.view(out.size(0), -1)     \n",
    "        #out = self.dropout(out)\n",
    "        # fully conntected 1\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        # dropout, p = 0.2\n",
    "        #out = self.dropout(out)\n",
    "        # fully connected 2 (Readout)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modified_Lenet_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Instantiate Loss Class\n",
    "- Convolutional Neural Network: **Cross Entropy Loss**\n",
    "    - _Feedforward Neural Network_: **Cross Entropy Loss**\n",
    "    - _Logistic Regression_: **Cross Entropy Loss**\n",
    "    - _Linear Regression_: **MSE**\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Instantiate Optimizer Class\n",
    "- Simplified equation\n",
    "    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $\n",
    "        - $\\theta$: parameters (our tensors with grad accumulation)\n",
    "        - $\\eta$: learning rate (how fast we want to learn)\n",
    "        - $\\nabla_\\theta$: parameters' gradients\n",
    "- Even simplier equation\n",
    "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    - **At every iteration, we update our model's parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters In-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x12aec1eb8>\n",
      "18\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([6])\n",
      "torch.Size([6])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 16, 5, 5])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "\n",
    "print(len(list(model.parameters())))\n",
    "\n",
    "for ii in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[ii].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Model Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             156\n",
      "       BatchNorm2d-2            [-1, 6, 28, 28]              12\n",
      "           Dropout-3            [-1, 6, 28, 28]               0\n",
      "              ReLU-4            [-1, 6, 28, 28]               0\n",
      "         MaxPool2d-5            [-1, 6, 14, 14]               0\n",
      "            Conv2d-6           [-1, 16, 10, 10]           2,416\n",
      "       BatchNorm2d-7           [-1, 16, 10, 10]              32\n",
      "           Dropout-8           [-1, 16, 10, 10]               0\n",
      "              ReLU-9           [-1, 16, 10, 10]               0\n",
      "        MaxPool2d-10             [-1, 16, 5, 5]               0\n",
      "           Conv2d-11            [-1, 120, 1, 1]          48,120\n",
      "      BatchNorm2d-12            [-1, 120, 1, 1]             240\n",
      "          Dropout-13            [-1, 120, 1, 1]               0\n",
      "             ReLU-14            [-1, 120, 1, 1]               0\n",
      "           Linear-15                   [-1, 84]          10,164\n",
      "             ReLU-16                   [-1, 84]               0\n",
      "           Linear-17                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 61,990\n",
      "Trainable params: 61,990\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.21\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.45\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanghongming/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train Model\n",
    "- Process \n",
    "    1. **Convert inputs/labels to tensors with grad accumulation**\n",
    "        - CNN Input: (1, 28, 28) \n",
    "        - Feedforward NN Input: (1, 28*28)\n",
    "    2. Clear gradient buffets\n",
    "    3. Get output given inputs \n",
    "    4. Get loss\n",
    "    5. Get gradients w.r.t. parameters\n",
    "    6. Update parameters using gradients\n",
    "        - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    7. REPEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:53<04:29, 53.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Training Loss: 0.38648587465286255. Validation Loss: 0.4148566722869873. Testing Loss: 0.36264681816101074. Accuracy: 92.10199737548828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [01:42<03:23, 50.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000. Training Loss: 0.13848236203193665. Validation Loss: 0.1938658505678177. Testing Loss: 0.15596233308315277. Accuracy: 95.31400299072266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [02:32<02:31, 50.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1500. Training Loss: 0.10431603342294693. Validation Loss: 0.12278994917869568. Testing Loss: 0.07425116002559662. Accuracy: 96.33599853515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [03:26<01:43, 51.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2000. Training Loss: 0.056925222277641296. Validation Loss: 0.09202762693166733. Testing Loss: 0.08308380097150803. Accuracy: 96.86000061035156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [04:16<00:51, 51.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2500. Training Loss: 0.09101399779319763. Validation Loss: 0.14916306734085083. Testing Loss: 0.09890991449356079. Accuracy: 97.22200012207031\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "# accuracy\n",
    "train_Acc_iter = []\n",
    "val_Acc_iter = []\n",
    "test_Acc_iter = []\n",
    "# loss\n",
    "train_Loss_iter = []\n",
    "val_Loss_iter = []\n",
    "test_Loss_iter = []\n",
    "\n",
    "iter_tem = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images\n",
    "        images = images.requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Iterate through train dataset\n",
    "                for images, labels in train_loader:\n",
    "                    # Load images\n",
    "                    images = images.requires_grad_()\n",
    "                \n",
    "                    # Forward pass only to get logits/output\n",
    "                    outputs = model(images)\n",
    "                \n",
    "                    # Get predictions from the maximum value\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                    # Total number of labels\n",
    "                    total += labels.size(0)\n",
    "                \n",
    "                    # Total correct predictions\n",
    "                    correct += (predicted == labels).sum()\n",
    "                \n",
    "                    # Testing loss\n",
    "                    # Calculate Loss: softmax --> cross entropy loss\n",
    "                    train_loss = criterion(outputs, labels)\n",
    "            \n",
    "                train_accuracy = 100 * correct / total\n",
    "                \n",
    "                \n",
    "                # Iterate through test dataset\n",
    "                for images, labels in test_loader:\n",
    "                    # Load images\n",
    "                    images = images.requires_grad_()\n",
    "                \n",
    "                    # Forward pass only to get logits/output\n",
    "                    outputs = model(images)\n",
    "                \n",
    "                    # Get predictions from the maximum value\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                    # Total number of labels\n",
    "                    test_total += labels.size(0)\n",
    "                \n",
    "                    # Total correct predictions\n",
    "                    test_correct += (predicted == labels).sum()\n",
    "                \n",
    "                    # Testing loss\n",
    "                    # Calculate Loss: softmax --> cross entropy loss\n",
    "                    test_loss = criterion(outputs, labels)\n",
    "            \n",
    "                test_accuracy = 100 * test_correct / test_total\n",
    "            \n",
    "                # Iterate through validation dataset\n",
    "                for images, labels in val_loader:\n",
    "                    # Load images\n",
    "                    images = images.requires_grad_()\n",
    "                \n",
    "                    # Forward pass only to get logits/output\n",
    "                    outputs = model(images)\n",
    "                \n",
    "                    # Get predictions from the maximum value\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                    # Total number of labels\n",
    "                    val_total += labels.size(0)\n",
    "                \n",
    "                    # Total correct predictions\n",
    "                    val_correct += (predicted == labels).sum()\n",
    "        \n",
    "                    # Calculate Loss: softmax --> cross entropy loss\n",
    "                    val_loss = criterion(outputs, labels)\n",
    "            \n",
    "                val_accuracy = 100 * val_correct / val_total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Training Loss: {}. Validation Loss: {}. Testing Loss: {}. Accuracy: {}'.format(iter, train_loss.item(),val_loss.item(),test_loss.item(), train_accuracy))\n",
    "            # accuracy\n",
    "            train_Acc_iter.append(train_accuracy)\n",
    "            val_Acc_iter.append(val_accuracy)\n",
    "            test_Acc_iter.append(test_accuracy)\n",
    "            # loss\n",
    "            train_Loss_iter.append(train_loss.item())\n",
    "            val_Loss_iter.append(val_loss.item())\n",
    "            test_Loss_iter.append(test_loss.item())\n",
    "            iter_tem.append(iter)\n",
    "# save Acc_iter, Loss_iter\n",
    "# Acc\n",
    "train_Acc_iter_numpy = np.asarray(train_Acc_iter)\n",
    "val_Acc_iter_numpy = np.asarray(val_Acc_iter)\n",
    "test_Acc_iter_numpy = np.asarray(test_Acc_iter)\n",
    "# Loss\n",
    "train_Loss_iter_numpy = np.asarray(train_Loss_iter)\n",
    "val_Loss_iter_numpy = np.asarray(val_Loss_iter)\n",
    "test_Loss_iter_numpy = np.asarray(test_Loss_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load data\n",
    "# 1. Acc\n",
    "train_acc = train_Acc_iter_numpy\n",
    "val_acc = val_Acc_iter_numpy\n",
    "test_acc = test_Acc_iter_numpy\n",
    "# 2. Loss\n",
    "train_loss = train_Loss_iter_numpy\n",
    "val_loss = val_Loss_iter_numpy\n",
    "test_loss = test_Loss_iter_numpy\n",
    "# Combined Plot\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(14,6))\n",
    "# Fig.1: Loss\n",
    "# Loss for the four optimizers\n",
    "ax1.plot(iter_tem,train_loss,color='red',label='Training')\n",
    "ax1.plot(iter_tem,val_loss,color='purple',label='Validation')\n",
    "ax1.plot(iter_tem,test_loss,color='green',label='Testing')\n",
    "# Label names\n",
    "ax1.set_xlabel(\"Iteration no.\",fontsize=14)\n",
    "ax1.set_ylabel(\"Cross Entropy Loss\",fontsize=14)\n",
    "# Tick size\n",
    "ax1.tick_params(axis='both', which='major', labelsize=14)\n",
    "# Legend position\n",
    "ax1.legend(loc='best',fontsize=14)\n",
    "# Axis scale\n",
    "ax1.set_yscale('log')\n",
    "# Fig.2: Accuracy\n",
    "ax2.plot(iter_tem,train_acc,color='red',label='Training')\n",
    "ax2.plot(iter_tem,val_acc,color='purple',label='Validation')\n",
    "ax2.plot(iter_tem,test_acc,color='green',label='Testing')\n",
    "# label name\n",
    "ax2.set_xlabel(\"Iteration no.\",fontsize=14)\n",
    "ax2.set_ylabel(\"Accuracy ($\\%$)\",\n",
    "               fontsize=14)\n",
    "# Tick size\n",
    "ax2.tick_params(axis='both', which='major', labelsize=14)\n",
    "# Legend position\n",
    "ax2.legend(loc='best',fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
